# -*- coding: utf-8 -*-
"""Assignment - 3 : Natural Language Translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mlWcC_RCEIqDDuBISyZOffNF52cFGK5u
"""

!pip install fasttext

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as npc
import pandas as pd
from tqdm import tqdm
import os
import random
from typing import List, Tuple, Dict
import pickle
import math
import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
import fasttext
import fasttext.util
import matplotlib.pyplot as plt
import numpy as np


# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True

# Constants
MAX_LENGTH = 128
BATCH_SIZE = 32
EMBEDDING_DIM = 300
HIDDEN_DIM = 512
NUM_LAYERS = 2
DROPOUT = 0.1
LEARNING_RATE = 0.001
NUM_EPOCHS = 5
DEVICE = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')
N_HEADS = 4

# Special tokens
SOS_TOKEN = '<sos>'
EOS_TOKEN = '<eos>'
PAD_TOKEN = '<pad>'
UNK_TOKEN = '<unk>'

class FastTextEmbedding:
    """Class to handle FastText embeddings for both English and Hindi"""
    def __init__(self, en_model_path='cc.en.300.bin', hi_model_path='cc.hi.300.bin'):
        try:
            print("Loading English FastText model...")
            self.en_model = fasttext.load_model(en_model_path)
        except:
            print("Downloading English FastText model...")
            fasttext.util.download_model('en', if_exists='ignore')
            self.en_model = fasttext.load_model('cc.en.300.bin')

        try:
            print("Loading Hindi FastText model...")
            self.hi_model = fasttext.load_model(hi_model_path)
        except:
            print("Downloading Hindi FastText model...")
            fasttext.util.download_model('hi', if_exists='ignore')
            self.hi_model = fasttext.load_model('cc.hi.300.bin')
        self.en_vocab = {}
        self.hi_vocab = {}
        self.en_embeddings = None
        self.hi_embeddings = None
        self.special_tokens = [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN]

    def build_vocab(self, en_sentences, hi_sentences):
        """Build vocabulary and embedding matrices from sentences"""
        print("Building English vocabulary...")
        en_words = set()
        for sent in en_sentences:
            for word in nltk.word_tokenize(sent):
                en_words.add(word.lower())

        print("Building Hindi vocabulary...")
        hi_words = set()
        for sent in hi_sentences:
            for word in nltk.word_tokenize(sent):
                hi_words.add(word)
        en_word_list = list(en_words)
        hi_word_list = list(hi_words)
        self.en_vocab = {token: idx for idx, token in enumerate(self.special_tokens)}
        for word in en_word_list:
            if word not in self.en_vocab:
                self.en_vocab[word] = len(self.en_vocab)
        self.hi_vocab = {token: idx for idx, token in enumerate(self.special_tokens)}
        for word in hi_word_list:
            if word not in self.hi_vocab:
                self.hi_vocab[word] = len(self.hi_vocab)

        print(f"Creating English embedding matrix for {len(self.en_vocab)} words...")
        self.en_embeddings = torch.zeros(len(self.en_vocab), EMBEDDING_DIM)
        for word, idx in self.en_vocab.items():
            if word in self.special_tokens:
                # Initialize special tokens with random vectors
                self.en_embeddings[idx] = torch.randn(EMBEDDING_DIM)
            else:
                self.en_embeddings[idx] = torch.tensor(self.en_model.get_word_vector(word))

        print(f"Creating Hindi embedding matrix for {len(self.hi_vocab)} words...")
        self.hi_embeddings = torch.zeros(len(self.hi_vocab), EMBEDDING_DIM)
        for word, idx in self.hi_vocab.items():
            if word in self.special_tokens:
                # Initialize special tokens with random vectors
                self.hi_embeddings[idx] = torch.randn(EMBEDDING_DIM)
            else:
                self.hi_embeddings[idx] = torch.tensor(self.hi_model.get_word_vector(word))

        print(f"English vocabulary size: {len(self.en_vocab)}")
        print(f"Hindi vocabulary size: {len(self.hi_vocab)}")

    def encode_sentence(self, sentence, vocab, max_length):
        """Encode a sentence to a list of indices using the vocabulary"""
        tokens = nltk.word_tokenize(sentence)
        indices = []

        for token in tokens:
            if token in vocab:
                indices.append(vocab[token])
            else:
                indices.append(vocab[UNK_TOKEN])
        if len(indices) < max_length:
            indices = indices + [vocab[PAD_TOKEN]] * (max_length - len(indices))
        else:
            indices = indices[:max_length]

        return torch.tensor(indices)

    def decode_sentence(self, indices, vocab):
        """Decode a list of indices to a sentence using the vocabulary"""

        idx_to_word = {idx: word for word, idx in vocab.items()}

        words = []
        for idx in indices:
            if isinstance(idx, torch.Tensor):
                idx = idx.item()
            word = idx_to_word.get(idx, UNK_TOKEN)
            if word not in [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]:
                words.append(word)
            elif word == EOS_TOKEN:
                break

        return ' '.join(words)

class TranslationDataset(Dataset):
    def __init__(self, english_file, hindi_file, embedding_handler, max_length):
        self.english_sentences = []
        self.hindi_sentences = []
        self.embedding_handler = embedding_handler
        self.max_length = max_length

        with open(english_file, 'r', encoding='utf-8') as f_en, \
             open(hindi_file, 'r', encoding='utf-8') as f_hi:
            for en_line, hi_line in zip(f_en, f_hi):
                en_sentence = SOS_TOKEN + ' ' + en_line.strip() + ' ' + EOS_TOKEN
                hi_sentence = SOS_TOKEN + ' ' + hi_line.strip() + ' ' + EOS_TOKEN

                self.english_sentences.append(en_sentence)
                self.hindi_sentences.append(hi_sentence)

    def __len__(self):
        return len(self.english_sentences)

    def __getitem__(self, idx):
        en_sentence = self.english_sentences[idx]
        hi_sentence = self.hindi_sentences[idx]

        # Encode sentences using our vocabulary
        en_indices = self.embedding_handler.encode_sentence(
            en_sentence,
            self.embedding_handler.en_vocab,
            self.max_length
        )

        hi_indices = self.embedding_handler.encode_sentence(
            hi_sentence,
            self.embedding_handler.hi_vocab,
            self.max_length
        )
        en_mask = (en_indices != self.embedding_handler.en_vocab[PAD_TOKEN]).float()
        hi_mask = (hi_indices != self.embedding_handler.hi_vocab[PAD_TOKEN]).float()

        return {
            'input_ids': en_indices,
            'attention_mask': en_mask,
            'target_ids': hi_indices,
            'target_attention_mask': hi_mask
        }

class EncoderLSTM(nn.Module):
    def __init__(self, embedding_matrix, hidden_dim, num_layers, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        vocab_size, embedding_dim = embedding_matrix.shape
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids):
        # input_ids shape: [batch_size, seq_len]
        embedded = self.dropout(self.embedding(input_ids))
        # embedded shape: [batch_size, seq_len, embedding_dim]

        # Forward pass through LSTM
        outputs, (hidden, cell) = self.lstm(embedded)
        batch_size = input_ids.size(0)
        decoder_hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(input_ids.device)
        decoder_cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(input_ids.device)
        for layer in range(self.num_layers):
            h_forward = hidden[2 * layer]
            h_backward = hidden[2 * layer + 1]
            h_combined = torch.cat((h_forward, h_backward), dim=1)
            decoder_hidden[layer] = self.fc(h_combined)
            c_forward = cell[2 * layer]
            c_backward = cell[2 * layer + 1]
            c_combined = torch.cat((c_forward, c_backward), dim=1)
            decoder_cell[layer] = self.fc(c_combined)

        return outputs, (decoder_hidden, decoder_cell)

class AttentionLayer(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        batch_size = encoder_outputs.size(0)
        src_len = encoder_outputs.size(1)
        if hidden.dim() != 2:
            if hidden.dim() == 3:
                hidden = hidden[0]
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        energy = torch.cat((hidden, encoder_outputs), dim=2)
        energy = torch.tanh(self.attn(energy))
        attention = self.v(energy).squeeze(2)
        attention_weights = torch.softmax(attention, dim=1)
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
        return context.squeeze(1), attention_weights

class DecoderLSTM(nn.Module):
    def __init__(self, embedding_matrix, hidden_dim, num_layers, dropout, attention=None):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.attention = attention

        vocab_size, embedding_dim = embedding_matrix.shape

        # Use pre-trained embeddings
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)
        self.lstm = nn.LSTM(
            embedding_dim + hidden_dim * 2 if attention else embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )

        self.fc_out = nn.Linear(hidden_dim * 3 if attention else hidden_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids, hidden, encoder_outputs=None):
        # Embed input
        embedded = self.dropout(self.embedding(input_ids))
        attn_weights = None
        if self.attention and encoder_outputs is not None:
            attn_hidden = hidden[0]
            context, attn_weights = self.attention(attn_hidden, encoder_outputs)
            embedded = torch.cat((embedded, context.unsqueeze(1)), dim=2)
        # Pass through LSTM
        output, (hidden, cell) = self.lstm(embedded, (hidden, hidden))  # Using hidden for both h and c
        if self.attention and encoder_outputs is not None:
            # Get output from last layer
            output_squeeze = output.squeeze(1)
            # Get output vector
            prediction = self.fc_out(torch.cat((output_squeeze, context), dim=1))
        else:
            prediction = self.fc_out(output.squeeze(1))
        return prediction, (hidden, cell), attn_weights

class Seq2SeqLSTM(nn.Module):
    def __init__(self, encoder, decoder, embedding_handler):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.embedding_handler = embedding_handler
        self.device = next(self.parameters()).device
        self.pad_idx = embedding_handler.hi_vocab[PAD_TOKEN]
        self.sos_idx = embedding_handler.hi_vocab[SOS_TOKEN]
        self.eos_idx = embedding_handler.hi_vocab[EOS_TOKEN]

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        batch_size = source.size(0)
        target_len = target.size(1)
        vocab_size = len(self.embedding_handler.hi_vocab)

        # Tensor to store outputs
        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)

        # Get encoder outputs and hidden state
        encoder_outputs, (hidden, cell) = self.encoder(source)

        # First input to the decoder is the <sos> token
        decoder_input = target[:, 0].unsqueeze(1)  # [batch_size, 1]

        for t in range(1, target_len):
            # Pass through decoder
            prediction, (hidden, cell), _ = self.decoder(decoder_input, hidden, encoder_outputs)

            # Store prediction
            outputs[:, t] = prediction

            # Decide whether to use teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio

            # Get the highest predicted token
            top1 = prediction.argmax(1).unsqueeze(1)

            # Use either ground truth or predicted token as next input
            decoder_input = target[:, t].unsqueeze(1) if teacher_force else top1

        return outputs

    def translate(self, src, max_len=100):
        batch_size = src.size(0)

        # Encode source sequence
        encoder_outputs, (hidden, cell) = self.encoder(src)

        # Start with SOS token
        decoder_input = torch.full((batch_size, 1),
                                   self.sos_idx,
                                   device=self.device).long()

        # Initialize arrays to store outputs
        output_ids = []
        attentions = []

        for i in range(max_len):
            # Pass through decoder
            prediction, (hidden, cell), attention = self.decoder(
                decoder_input, hidden, encoder_outputs)

            # Get most likely next token
            top1 = prediction.argmax(1).unsqueeze(1)
            output_ids.append(top1)

            if attention is not None:
                attentions.append(attention)

            # Check if all batches have generated EOS token
            if (top1 == self.eos_idx).all():
                break

            # Use as next input
            decoder_input = top1

        # Concatenate all outputs
        output_tensor = torch.cat(output_ids, dim=1)

        # If we have attention, return it too
        if attentions:
            attention_tensor = torch.cat(attentions, dim=1)
            return output_tensor, attention_tensor

        return output_tensor

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x.transpose(0, 1)
        x = x + self.pe[:x.size(0)]
        return x.transpose(0, 1)

class TransformerEncoder(nn.Module):
    def __init__(self, embedding_matrix, d_model, nhead, num_layers, dim_feedforward, dropout):
        super().__init__()
        # Use pre-trained embeddings
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)
        self.pos_encoder = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None, src_key_padding_mask=None):
        # src shape: [batch_size, src_len]
        src = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)
        # src shape: [batch_size, src_len, embedding_dim]

        src = self.pos_encoder(src)
        src = self.dropout(src)

        return self.transformer_encoder(src, src_mask, src_key_padding_mask)

class TransformerDecoder(nn.Module):
    def __init__(self, embedding_matrix, d_model, nhead, num_layers, dim_feedforward, dropout):
        super().__init__()
        vocab_size, _ = embedding_matrix.shape

        # Use pre-trained embeddings
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)
        self.pos_encoder = PositionalEncoding(d_model)
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,
                tgt_key_padding_mask=None, memory_key_padding_mask=None):
        tgt = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)
        tgt = self.pos_encoder(tgt)
        tgt = self.dropout(tgt)
        output = self.transformer_decoder(tgt, memory, tgt_mask, memory_mask,
                                          tgt_key_padding_mask, memory_key_padding_mask)
        return self.fc_out(output)

class Seq2SeqTransformer(nn.Module):
    def __init__(self, encoder, decoder, embedding_handler):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.embedding_handler = embedding_handler
        self.device = next(self.parameters()).device
        self.pad_idx = embedding_handler.hi_vocab[PAD_TOKEN]
        self.sos_idx = embedding_handler.hi_vocab[SOS_TOKEN]
        self.eos_idx = embedding_handler.hi_vocab[EOS_TOKEN]

    def generate_square_subsequent_mask(self, sz):
        """Generate a square mask for the sequence."""
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask.to(self.device)

    def create_padding_mask(self, matrix, pad_token):
        """Create mask for padding tokens."""
        return (matrix == pad_token)

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        batch_size = source.size(0)
        target_len = target.size(1)
        vocab_size = len(self.embedding_handler.hi_vocab)
        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)
        src_padding_mask = self.create_padding_mask(source, self.pad_idx)
        memory = self.encoder(source, src_key_padding_mask=src_padding_mask)
        decoder_input = target[:, :1]

        for t in range(1, target_len):
            tgt_mask = self.generate_square_subsequent_mask(decoder_input.size(1))
            tgt_padding_mask = self.create_padding_mask(decoder_input, self.pad_idx)
            output = self.decoder(
                decoder_input,
                memory,
                tgt_mask=tgt_mask,
                tgt_key_padding_mask=tgt_padding_mask,
                memory_key_padding_mask=src_padding_mask
            )
            outputs[:, t] = output[:, -1]
            teacher_force = random.random() < teacher_forcing_ratio
            if teacher_force:
                decoder_input = target[:, :t+1]
            else:
                next_token = output[:, -1].argmax(dim=1, keepdim=True)
                decoder_input = torch.cat([decoder_input, next_token], dim=1)
        return outputs

    def translate(self, src, max_len=100):
        batch_size = src.size(0)
        src_padding_mask = self.create_padding_mask(src, self.pad_idx)
        memory = self.encoder(src, src_key_padding_mask=src_padding_mask)
        decoder_input = torch.full((batch_size, 1), self.sos_idx, device=self.device).long()

        for i in range(max_len - 1):
            tgt_mask = self.generate_square_subsequent_mask(decoder_input.size(1))
            tgt_padding_mask = self.create_padding_mask(decoder_input, self.pad_idx)
            # Decode
            output = self.decoder(
                decoder_input,
                memory,
                tgt_mask=tgt_mask,
                tgt_key_padding_mask=tgt_padding_mask,
                memory_key_padding_mask=src_padding_mask
            )
            next_token = output[:, -1].argmax(dim=1, keepdim=True)
            decoder_input = torch.cat([decoder_input, next_token], dim=1)
            if (next_token == self.eos_idx).all():
                break
        return decoder_input

def train_epoch(model, data_loader, optimizer, criterion, device, model_type, epoch, teacher_forcing_ratio=0.5):
    model.train()
    total_loss = 0

    progress_bar = tqdm(data_loader, desc=f'Training {model_type} (Epoch {epoch})')

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        target_ids = batch['target_ids'].to(device)
        optimizer.zero_grad()
        output = model(input_ids, target_ids, teacher_forcing_ratio)
        output_dim = output.shape[-1]
        output = output[:, 1:].contiguous().view(-1, output_dim)
        target = target_ids[:, 1:].contiguous().view(-1)
        output = output.to(device)
        target = target.to(device)
        loss = criterion(output, target)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())
    return total_loss / len(data_loader)

def evaluate(model, data_loader, criterion, device, embedding_handler, model_type):
    model.eval()
    total_loss = 0
    references = []
    hypotheses = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc=f'Evaluating {model_type}'):
            input_ids = batch['input_ids'].to(device)
            target_ids = batch['target_ids'].to(device)
            if hasattr(model, 'translate'):
                translations = model.translate(input_ids)
                for i in range(input_ids.size(0)):
                    target_sentence = target_ids[i].cpu().tolist()
                    try:
                        eos_idx = target_sentence.index(embedding_handler.hi_vocab[EOS_TOKEN])
                        target_sentence = target_sentence[1:eos_idx]
                    except ValueError:
                        pad_id = embedding_handler.hi_vocab[PAD_TOKEN]
                        target_sentence = [t for t in target_sentence[1:] if t != pad_id]
                    if isinstance(translations, tuple):
                        pred_sentence = translations[0][i].cpu().tolist()
                    else:
                        pred_sentence = translations[i].cpu().tolist()
                    try:
                        eos_idx = pred_sentence.index(embedding_handler.hi_vocab[EOS_TOKEN])
                        pred_sentence = pred_sentence[1:eos_idx]
                    except ValueError:
                        pred_sentence = pred_sentence[1:]

                    # Create word lists from token IDs
                    # We need to map indices back to words
                    idx_to_word = {idx: word for word, idx in embedding_handler.hi_vocab.items()}
                    target_words = [idx_to_word.get(idx, UNK_TOKEN) for idx in target_sentence]
                    pred_words = [idx_to_word.get(idx, UNK_TOKEN) for idx in pred_sentence]

                    # Append to lists for BLEU calculation
                    references.append([target_words])  # BLEU expects list of lists
                    hypotheses.append(pred_words)

            # Forward pass for loss calculation
            output = model(input_ids, target_ids, 0.0)

            # Reshape for loss calculation
            output_dim = output.shape[-1]
            output = output[:, 1:].contiguous().view(-1, output_dim)
            target = target_ids[:, 1:].contiguous().view(-1)

            # Make sure both are on the same device
            output = output.to(device)
            target = target.to(device)

            # Calculate loss
            loss = criterion(output, target)
            total_loss += loss.item()

    # Calculate BLEU score using smoothing function
    bleu_score = corpus_bleu(
        references,
        hypotheses,
        weights=(0.25, 0.25, 0.25, 0.25),  # Use 4-gram BLEU
        smoothing_function=SmoothingFunction().method1
    )

    return total_loss / len(data_loader), bleu_score

def plot_training_progress(train_losses, test_losses, bleu_scores, model_type):
    """
    Plot training and test losses as well as BLEU scores.

    Args:
        train_losses: List of training losses per epoch
        test_losses: List of test losses per epoch
        bleu_scores: List of BLEU scores per epoch
        model_type: String indicating the model type (LSTM or Transformer)
    """
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 8))

    # Plot training and test loss
    plt.subplot(2, 1, 1)
    plt.plot(epochs, train_losses, 'b-', label='Training Loss')
    plt.plot(epochs, test_losses, 'r-', label='Test Loss')
    plt.title(f'{model_type} Model - Training and Test Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot BLEU scores
    plt.subplot(2, 1, 2)
    plt.plot(epochs, bleu_scores, 'g-', label='BLEU Score')
    plt.title(f'{model_type} Model - BLEU Score')
    plt.xlabel('Epochs')
    plt.ylabel('BLEU Score')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(f'{model_type.lower()}_training_progress.png')
    plt.close()

    print(f"Training progress plot saved as {model_type.lower()}_training_progress.png")

def translate_with_model(model_path, embedding_handler, source_sentences, model_type='lstm'):
    """
    Translate English sentences to Hindi using a saved model.

    Args:
        model_path: Path to the saved model weights
        embedding_handler: FastTextEmbedding instance with vocabulary and embeddings
        source_sentences: List of English sentences to translate
        model_type: 'lstm' or 'transformer'

    Returns:
        List of translated Hindi sentences
    """
    # Use CPU for inference to avoid device conflicts
    device = torch.device('cpu')

    # Make sure embedding matrices are on the correct device
    en_embeddings_device = embedding_handler.en_embeddings.to(device)
    hi_embeddings_device = embedding_handler.hi_embeddings.to(device)

    # Determine which model to load
    if model_type.lower() == 'lstm':
        # Initialize encoder with FastText embeddings
        encoder = EncoderLSTM(
            embedding_matrix=en_embeddings_device,
            hidden_dim=HIDDEN_DIM,
            num_layers=NUM_LAYERS,
            dropout=DROPOUT
        )

        # Initialize attention layer
        attention = AttentionLayer(HIDDEN_DIM)

        # Initialize decoder with FastText embeddings
        decoder = DecoderLSTM(
            embedding_matrix=hi_embeddings_device,
            hidden_dim=HIDDEN_DIM,
            num_layers=NUM_LAYERS,
            dropout=DROPOUT,
            attention=attention
        )

        # Create Seq2Seq model
        model = Seq2SeqLSTM(encoder, decoder, embedding_handler)
    else:
        # Initialize encoder with FastText embeddings
        encoder = TransformerEncoder(
            embedding_matrix=en_embeddings_device,
            d_model=EMBEDDING_DIM,
            nhead=N_HEADS,
            num_layers=NUM_LAYERS,
            dim_feedforward=HIDDEN_DIM,
            dropout=DROPOUT
        )

        # Initialize decoder with FastText embeddings
        decoder = TransformerDecoder(
            embedding_matrix=hi_embeddings_device,
            d_model=EMBEDDING_DIM,
            nhead=N_HEADS,
            num_layers=NUM_LAYERS,
            dim_feedforward=HIDDEN_DIM,
            dropout=DROPOUT
        )

        # Create Seq2Seq model
        model = Seq2SeqTransformer(encoder, decoder, embedding_handler)

    # Load saved weights and ensure model is on CPU
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    # Prepare source sentences
    processed_inputs = []
    for sentence in source_sentences:
        # Add SOS and EOS tokens
        processed_sentence = SOS_TOKEN + ' ' + sentence + ' ' + EOS_TOKEN

        # Encode sentence
        encoded = embedding_handler.encode_sentence(
            processed_sentence,
            embedding_handler.en_vocab,
            MAX_LENGTH
        )

        processed_inputs.append(encoded)

    # Stack inputs into a batch
    input_tensor = torch.stack(processed_inputs).to(device)

    # Translate
    with torch.no_grad():
        translation_result = model.translate(input_tensor)

    # Handle the result which could be either a tensor or a tuple (output_tensor, attention)
    if isinstance(translation_result, tuple):
        output_tensor = translation_result[0]  # Extract the output tensor from the tuple
    else:
        output_tensor = translation_result

    # Decode translated sentences
    translations = []
    for i in range(output_tensor.size(0)):
        output_ids = output_tensor[i].cpu().tolist()
        translated = embedding_handler.decode_sentence(output_ids, embedding_handler.hi_vocab)
        translations.append(translated)

    return translations

# Function to conduct ablation study
def ablation_study(train_loader, test_loader, embedding_handler, criterion, device):
    """
    Perform ablation study by comparing different model configurations.
    """
    print("\nPerforming Ablation Study...")

    # Define configurations to test
    configs = [
        {
            "name": "LSTM-Base",
            "encoder_layers": 2,
            "decoder_layers": 2,
            "hidden_dim": 512,
            "use_attention": True,
            "model_type": "lstm"
        },
        {
            "name": "LSTM-No-Attention",
            "encoder_layers": 2,
            "decoder_layers": 2,
            "hidden_dim": 512,
            "use_attention": False,
            "model_type": "lstm"
        },
        {
            "name": "LSTM-Single-Layer",
            "encoder_layers": 1,
            "decoder_layers": 1,
            "hidden_dim": 512,
            "use_attention": True,
            "model_type": "lstm"
        },
        {
            "name": "LSTM-Larger-Hidden",
            "encoder_layers": 2,
            "decoder_layers": 2,
            "hidden_dim": 768,
            "use_attention": True,
            "model_type": "lstm"
        },
        {
            "name": "Transformer-Base",
            "encoder_layers": 2,
            "decoder_layers": 2,
            "hidden_dim": 512,
            "nheads": 8,
            "model_type": "transformer"
        },
        {
            "name": "Transformer-Single-Layer",
            "encoder_layers": 1,
            "decoder_layers": 1,
            "hidden_dim": 512,
            "nheads": 8,
            "model_type": "transformer"
        },
        {
            "name": "Transformer-4-Heads",
            "encoder_layers": 2,
            "decoder_layers": 2,
            "hidden_dim": 512,
            "nheads": 4,
            "model_type": "transformer"
        },
        {
            "name": "Transformer-Larger-FFN",
            "encoder_layers": 2,
            "decoder_layers": 2,
            "hidden_dim": 1024,
            "nheads": 8,
            "model_type": "transformer"
        }
    ]

    results = []

    # Test each configuration
    for config in configs:
        print(f"\nTesting configuration: {config['name']}")

        if config["model_type"] == "lstm":
            # Create LSTM model with specified configuration
            encoder = EncoderLSTM(
                embedding_matrix=embedding_handler.en_embeddings,
                hidden_dim=config["hidden_dim"],
                num_layers=config["encoder_layers"],
                dropout=DROPOUT
            )

            attention = AttentionLayer(config["hidden_dim"]) if config["use_attention"] else None

            decoder = DecoderLSTM(
                embedding_matrix=embedding_handler.hi_embeddings,
                hidden_dim=config["hidden_dim"],
                num_layers=config["decoder_layers"],
                dropout=DROPOUT,
                attention=attention
            )

            model = Seq2SeqLSTM(encoder, decoder, embedding_handler).to(device)

        else:  # transformer
            # Create Transformer model with specified configuration
            encoder = TransformerEncoder(
                embedding_matrix=embedding_handler.en_embeddings,
                d_model=EMBEDDING_DIM,
                nhead=config["nheads"],
                num_layers=config["encoder_layers"],
                dim_feedforward=config["hidden_dim"],
                dropout=DROPOUT
            )

            decoder = TransformerDecoder(
                embedding_matrix=embedding_handler.hi_embeddings,
                d_model=EMBEDDING_DIM,
                nhead=config["nheads"],
                num_layers=config["decoder_layers"],
                dim_feedforward=config["hidden_dim"],
                dropout=DROPOUT
            )

            model = Seq2SeqTransformer(encoder, decoder, embedding_handler).to(device)

        # Initialize optimizer
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

        # Train for a few epochs
        for epoch in range(3):  # Reduce to save time
            train_loss = train_epoch(
                model,
                train_loader,
                optimizer,
                criterion,
                device,
                config["name"],
                epoch+1,
                teacher_forcing_ratio=0.5
            )

            test_loss, bleu = evaluate(
                model,
                test_loader,
                criterion,
                device,
                embedding_handler,
                config["name"]
            )

            print(f'Epoch {epoch+1}/3:')
            print(f'Train Loss: {train_loss:.4f}')
            print(f'Test Loss: {test_loss:.4f}')
            print(f'BLEU Score: {bleu:.4f}')

        results.append({
            "configuration": config["name"],
            "bleu": bleu,
            "loss": test_loss
        })

    # Print comparison
    print("\nAblation Study Results:")
    print("-" * 50)
    print(f"{'Configuration':<25} {'BLEU':<10} {'Loss':<10}")
    print("-" * 50)

    for result in sorted(results, key=lambda x: x["bleu"], reverse=True):
        print(f"{result['configuration']:<25} {result['bleu']:<10.4f} {result['loss']:<10.4f}")

# Function to propose advanced model
def propose_advanced_model():
    """
    This function defines and returns an advanced model that combines
    the strengths of both LSTM and Transformer architectures.
    """
    class HybridTranslationModel(nn.Module):
        def __init__(self, embedding_handler, hidden_dim=512, num_layers=2, nheads=8, dropout=0.1):
            super().__init__()

            # Shared encoders for both architectures
            self.lstm_encoder = EncoderLSTM(
                embedding_matrix=embedding_handler.en_embeddings,
                hidden_dim=hidden_dim,
                num_layers=num_layers,
                dropout=dropout
            )

            self.transformer_encoder = TransformerEncoder(
                embedding_matrix=embedding_handler.en_embeddings,
                d_model=EMBEDDING_DIM,
                nhead=nheads,
                num_layers=num_layers,
                dim_feedforward=hidden_dim,
                dropout=dropout
            )

            # Fusion layer to combine encoder outputs
            self.fusion_layer = nn.Linear(hidden_dim * 2 + EMBEDDING_DIM, EMBEDDING_DIM)

            # Decoder (using transformer decoder)
            self.decoder = TransformerDecoder(
                embedding_matrix=embedding_handler.hi_embeddings,
                d_model=EMBEDDING_DIM,
                nhead=nheads,
                num_layers=num_layers,
                dim_feedforward=hidden_dim,
                dropout=dropout
            )

            self.embedding_handler = embedding_handler
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.pad_idx = embedding_handler.hi_vocab[PAD_TOKEN]
            self.sos_idx = embedding_handler.hi_vocab[SOS_TOKEN]
            self.eos_idx = embedding_handler.hi_vocab[EOS_TOKEN]

        def generate_square_subsequent_mask(self, sz):
            mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
            return mask.to(self.device)

        def create_padding_mask(self, matrix, pad_token):
            return (matrix == pad_token)

        def forward(self, source, target, teacher_forcing_ratio=0.5):
            batch_size = source.size(0)
            target_len = target.size(1)
            vocab_size = len(self.embedding_handler.hi_vocab)

            # Tensor to store outputs
            outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)

            # Create source padding mask
            src_padding_mask = self.create_padding_mask(source, self.pad_idx)

            # Get encoder outputs from both architectures
            lstm_outputs, (hidden, cell) = self.lstm_encoder(source)
            transformer_outputs = self.transformer_encoder(source, src_key_padding_mask=src_padding_mask)

            fused_memory = torch.cat((lstm_outputs, transformer_outputs), dim=2)
            fused_memory = self.fusion_layer(fused_memory)

            # Start with just the SOS token
            decoder_input = target[:, :1]

            for t in range(1, target_len):
                # Create target mask to prevent attention to future tokens
                tgt_mask = self.generate_square_subsequent_mask(decoder_input.size(1))

                # Create target padding mask
                tgt_padding_mask = self.create_padding_mask(decoder_input, self.pad_idx)

                # Decode
                output = self.decoder(
                    decoder_input,
                    fused_memory,
                    tgt_mask=tgt_mask,
                    tgt_key_padding_mask=tgt_padding_mask,
                    memory_key_padding_mask=src_padding_mask
                )

                # Store prediction
                outputs[:, t] = output[:, -1]

                # Decide whether to use teacher forcing
                teacher_force = random.random() < teacher_forcing_ratio

                if teacher_force:
                    # Add next ground truth token
                    decoder_input = target[:, :t+1]
                else:
                    # Add predicted token
                    next_token = output[:, -1].argmax(dim=1, keepdim=True)
                    decoder_input = torch.cat([decoder_input, next_token], dim=1)

            return outputs

        def translate(self, src, max_len=100):
            batch_size = src.size(0)

            # Create source padding mask
            src_padding_mask = self.create_padding_mask(src, self.pad_idx)

            # Get encoder outputs from both architectures
            lstm_outputs, (hidden, cell) = self.lstm_encoder(src)
            transformer_outputs = self.transformer_encoder(src, src_key_padding_mask=src_padding_mask)

            # Fuse the encoder outputs
            fused_memory = torch.cat((lstm_outputs, transformer_outputs), dim=2)
            fused_memory = self.fusion_layer(fused_memory)

            # Start with SOS token
            decoder_input = torch.full((batch_size, 1), self.sos_idx, device=self.device).long()

            for i in range(max_len - 1):
                # Create target mask
                tgt_mask = self.generate_square_subsequent_mask(decoder_input.size(1))

                # Create target padding mask
                tgt_padding_mask = self.create_padding_mask(decoder_input, self.pad_idx)

                # Decode
                output = self.decoder(
                    decoder_input,
                    fused_memory,
                    tgt_mask=tgt_mask,
                    tgt_key_padding_mask=tgt_padding_mask,
                    memory_key_padding_mask=src_padding_mask
                )

                # Get next token
                next_token = output[:, -1].argmax(dim=1, keepdim=True)
                decoder_input = torch.cat([decoder_input, next_token], dim=1)

                # Stop if all sequences have EOS token
                if (next_token == self.eos_idx).all():
                    break

            return decoder_input

    return HybridTranslationModel

def main(embedding_handler, train_dataset, test_dataset):
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

    # Initialize criterion (ignore padding index)
    criterion = nn.CrossEntropyLoss(ignore_index=embedding_handler.hi_vocab[PAD_TOKEN]).to(DEVICE)

    # Make sure embedding matrices are on the correct device
    embedding_handler.en_embeddings = embedding_handler.en_embeddings.to(DEVICE)
    embedding_handler.hi_embeddings = embedding_handler.hi_embeddings.to(DEVICE)

    # Train LSTM model
    print("\nTraining LSTM model...")

    # Initialize encoder with FastText embeddings
    lstm_encoder = EncoderLSTM(
        embedding_matrix=embedding_handler.en_embeddings,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT
    ).to(DEVICE)

    # Initialize attention layer
    attention = AttentionLayer(HIDDEN_DIM).to(DEVICE)

    # Initialize decoder with FastText embeddings
    lstm_decoder = DecoderLSTM(
        embedding_matrix=embedding_handler.hi_embeddings,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT,
        attention=attention
    ).to(DEVICE)

    # Create Seq2Seq model
    lstm_model = Seq2SeqLSTM(lstm_encoder, lstm_decoder, embedding_handler).to(DEVICE)

    # Initialize optimizer
    lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)

    # Lists to store metrics for plotting
    lstm_train_losses = []
    lstm_test_losses = []
    lstm_bleu_scores = []

    # Train model
    best_lstm_bleu = 0.0
    for epoch in range(NUM_EPOCHS):
        train_loss = train_epoch(
            lstm_model,
            train_loader,
            lstm_optimizer,
            criterion,
            DEVICE,
            "LSTM",
            epoch+1,
            teacher_forcing_ratio=0.5
        )

        test_loss, bleu = evaluate(
            lstm_model,
            test_loader,
            criterion,
            DEVICE,
            embedding_handler,
            "LSTM"
        )

        # Store metrics for plotting
        lstm_train_losses.append(train_loss)
        lstm_test_losses.append(test_loss)
        lstm_bleu_scores.append(bleu)

        print(f'Epoch {epoch+1}/{NUM_EPOCHS}:')
        print(f'Train Loss: {train_loss:.4f}')
        print(f'Test Loss: {test_loss:.4f}')
        print(f'BLEU Score: {bleu:.4f}')

        if bleu > best_lstm_bleu:
            best_lstm_bleu = bleu
            torch.save(lstm_model.state_dict(), 'best_lstm_model.pt')
            print(f"New best LSTM model saved with BLEU score: {bleu:.4f}")

    # Plot LSTM training progress
    plot_training_progress(lstm_train_losses, lstm_test_losses, lstm_bleu_scores, "LSTM")

    # Train Transformer model
    print("\nTraining Transformer model...")

    # Initialize encoder with FastText embeddings
    transformer_encoder = TransformerEncoder(
        embedding_matrix=embedding_handler.en_embeddings,
        d_model=EMBEDDING_DIM,
        nhead=N_HEADS,
        num_layers=NUM_LAYERS,
        dim_feedforward=HIDDEN_DIM,
        dropout=DROPOUT
    ).to(DEVICE)

    # Initialize decoder with FastText embeddings
    transformer_decoder = TransformerDecoder(
        embedding_matrix=embedding_handler.hi_embeddings,
        d_model=EMBEDDING_DIM,
        nhead=N_HEADS,
        num_layers=NUM_LAYERS,
        dim_feedforward=HIDDEN_DIM,
        dropout=DROPOUT
    ).to(DEVICE)

    # Create Seq2Seq model
    transformer_model = Seq2SeqTransformer(transformer_encoder, transformer_decoder, embedding_handler).to(DEVICE)

    # Initialize optimizer
    transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE)

    # Lists to store metrics for plotting
    transformer_train_losses = []
    transformer_test_losses = []
    transformer_bleu_scores = []

    # Train model
    best_transformer_bleu = 0.0
    for epoch in range(NUM_EPOCHS):
        train_loss = train_epoch(
            transformer_model,
            train_loader,
            transformer_optimizer,
            criterion,
            DEVICE,
            "Transformer",
            epoch+1,
            teacher_forcing_ratio=0.5
        )

        test_loss, bleu = evaluate(
            transformer_model,
            test_loader,
            criterion,
            DEVICE,
            embedding_handler,
            "Transformer"
        )

        # Store metrics for plotting
        transformer_train_losses.append(train_loss)
        transformer_test_losses.append(test_loss)
        transformer_bleu_scores.append(bleu)

        print(f'Epoch {epoch+1}/{NUM_EPOCHS}:')
        print(f'Train Loss: {train_loss:.4f}')
        print(f'Test Loss: {test_loss:.4f}')
        print(f'BLEU Score: {bleu:.4f}')

        if bleu > best_transformer_bleu:
            best_transformer_bleu = bleu
            torch.save(transformer_model.state_dict(), 'best_transformer_model.pt')
            print(f"New best Transformer model saved with BLEU score: {bleu:.4f}")

    # Plot Transformer training progress
    plot_training_progress(transformer_train_losses, transformer_test_losses, transformer_bleu_scores, "Transformer")

    # Print final results comparing both models
    print("\nFinal Results:")
    print(f"Best LSTM BLEU Score: {best_lstm_bleu:.4f}")
    print(f"Best Transformer BLEU Score: {best_transformer_bleu:.4f}")

    # Example of translation with the best model
    test_sentences = [
        "it is raining outside",
        "how are you today",
        "the weather is nice"
    ]

    print("\nTranslation examples using best LSTM model:")
    lstm_translations = translate_with_model('best_lstm_model.pt', embedding_handler, test_sentences, 'lstm')
    for src, tgt in zip(test_sentences, lstm_translations):
        print(f"English: {src}")
        print(f"Hindi: {tgt}")
        print()

    print("\nTranslation examples using best Transformer model:")
    transformer_translations = translate_with_model('best_transformer_model.pt', embedding_handler, test_sentences, 'transformer')
    for src, tgt in zip(test_sentences, transformer_translations):
        print(f"English: {src}")
        print(f"Hindi: {tgt}")
        print()

if __name__ == '__main__':

    en_train_file = './english.train'
    hi_train_file = './hindi.train'
    en_test_file = './english.test'
    hi_test_file = './hindi.test'

    # Load all sentences for vocabulary building
    en_sentences = []
    hi_sentences = []

    print(f"Loading English training data from {en_train_file}")
    with open(en_train_file, 'r', encoding='utf-8') as f:
        for line in f:
            en_sentences.append(line.strip())

    print(f"Loading Hindi training data from {hi_train_file}")
    with open(hi_train_file, 'r', encoding='utf-8') as f:
        for line in f:
            hi_sentences.append(line.strip())

    print(f"Loading English test data from {en_test_file}")
    with open(en_test_file, 'r', encoding='utf-8') as f:
        for line in f:
            en_sentences.append(line.strip())

    print(f"Loading Hindi test data from {hi_test_file}")
    with open(hi_test_file, 'r', encoding='utf-8') as f:
        for line in f:
            hi_sentences.append(line.strip())

    # Initialize FastText embedding handler
    embedding_handler = FastTextEmbedding()

    # Build vocabularies and embeddings
    embedding_handler.build_vocab(en_sentences, hi_sentences)

    # Create datasets
    train_dataset = TranslationDataset(en_train_file, hi_train_file, embedding_handler, MAX_LENGTH)
    test_dataset = TranslationDataset(en_test_file, hi_test_file, embedding_handler, MAX_LENGTH)

    main(embedding_handler, train_dataset, test_dataset)
